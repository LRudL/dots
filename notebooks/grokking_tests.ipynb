{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import math\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.utils.data as tdata\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "from dots.training import *\n",
    "from dots.trainhooks import *\n",
    "from dots.models import MLP\n",
    "from dots.dots import *\n",
    "from dots.utils import get_device\n",
    "from dots.plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import functools\n",
    "import itertools\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import einops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.W_E = nn.Parameter(torch.randn(d_model, d_vocab)/np.sqrt(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.einsum('dbp -> bpd', self.W_E[:, x])\n",
    "\n",
    "class Unembed(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.W_U = nn.Parameter(torch.randn(d_model, d_vocab)/np.sqrt(d_vocab))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (x @ self.W_U)\n",
    "\n",
    "\n",
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, max_ctx, d_model):\n",
    "        super().__init__()\n",
    "        self.W_pos = nn.Parameter(torch.randn(max_ctx, d_model)/np.sqrt(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x+self.W_pos[:x.shape[-2]]\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, epsilon = 1e-4, model=[None]):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.w_ln = nn.Parameter(torch.ones(d_model))\n",
    "        self.b_ln = nn.Parameter(torch.zeros(d_model))\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.model[0].use_ln:\n",
    "            x = x - x.mean(axis=-1)[..., None]\n",
    "            x = x / (x.std(axis=-1)[..., None] + self.epsilon)\n",
    "            x = x * self.w_ln\n",
    "            x = x + self.b_ln\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_head, n_ctx, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.W_K = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_Q = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_V = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_O = nn.Parameter(torch.randn(d_model, d_head * num_heads)/np.sqrt(d_model))\n",
    "        self.register_buffer('mask', torch.tril(torch.ones((n_ctx, n_ctx))))\n",
    "        self.d_head = d_head\n",
    "        \n",
    "    def forward(self, x):\n",
    "        k = torch.einsum('ihd,bpd->biph', self.W_K, x)\n",
    "        q = torch.einsum('ihd,bpd->biph', self.W_Q, x)\n",
    "        v = torch.einsum('ihd,bpd->biph', self.W_V, x)\n",
    "        attn_scores_pre = torch.einsum('biph,biqh->biqp', k, q)\n",
    "        attn_scores_masked = torch.tril(attn_scores_pre) - 1e10 * (1 - self.mask[:x.shape[-2], :x.shape[-2]])\n",
    "        attn_matrix = F.softmax(attn_scores_masked/np.sqrt(self.d_head), dim=-1)\n",
    "        z = torch.einsum('biph,biqp->biqh', v, attn_matrix)\n",
    "        z_flat = einops.rearrange(z, 'b i q h -> b q (i h)')\n",
    "        out = torch.einsum('df,bqf->bqd', self.W_O, z_flat)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, d_mlp, act_type, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.W_in = nn.Parameter(torch.randn(d_mlp, d_model)/np.sqrt(d_model))\n",
    "        self.b_in = nn.Parameter(torch.zeros(d_mlp))\n",
    "        self.W_out = nn.Parameter(torch.randn(d_model, d_mlp)/np.sqrt(d_model))\n",
    "        self.b_out = nn.Parameter(torch.zeros(d_model))\n",
    "        self.act_type = act_type\n",
    "        # self.ln = LayerNorm(d_mlp, model=self.model)\n",
    "        assert act_type in ['ReLU', 'GeLU']\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.einsum('md,bpd->bpm', self.W_in, x) + self.b_in\n",
    "        \n",
    "        if self.act_type=='ReLU':\n",
    "            x = F.relu(x)\n",
    "        elif self.act_type=='GeLU':\n",
    "            x = F.gelu(x)\n",
    "        \n",
    "        x = torch.einsum('dm,bpm->bpd', self.W_out, x) + self.b_out\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        # self.ln1 = LayerNorm(d_model, model=self.model)\n",
    "        self.attn = Attention(d_model, num_heads, d_head, n_ctx, model=self.model)\n",
    "        # self.ln2 = LayerNorm(d_model, model=self.model)\n",
    "        self.mlp = MLP(d_model, d_mlp, act_type, model=self.model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(x)\n",
    "        x = x + self.mlp((x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_vocab, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, use_ln=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = Embed(d_vocab, d_model)\n",
    "        self.pos_embed = PosEmbed(n_ctx, d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model=[self]) for i in range(num_layers)])\n",
    "        # self.ln = LayerNorm(d_model, model=[self])\n",
    "        self.unembed = Unembed(d_vocab, d_model)\n",
    "        self.use_ln = use_ln\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.pos_embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        # x = self.ln(x)\n",
    "        x = self.unembed(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr=1e-3 #@param\n",
    "weight_decay = 0.1 #@param \n",
    "p=113 #@param\n",
    "d_model = 128 #@param\n",
    "fn_name = 'add' #@param ['add', 'subtract', 'x2xyy2','rand']\n",
    "frac_train = 0.3 #@param\n",
    "NUM_EPOCHS = 100000 #@param\n",
    "save_models = False #@param\n",
    "save_every = 1000 #@param\n",
    "# Stop training when test loss is <stopping_thresh\n",
    "stopping_thresh = -1 #@param\n",
    "seed = 0 #@param\n",
    "num_layers = 1\n",
    "batch_style = 'full'\n",
    "d_vocab = p+1 \n",
    "n_ctx = 3\n",
    "d_mlp = 4*d_model\n",
    "num_heads = 4\n",
    "assert d_model % num_heads == 0\n",
    "d_head = d_model//num_heads\n",
    "act_type = 'ReLU' #@param ['ReLU', 'GeLU']\n",
    "#batch_size = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3830 8939\n"
     ]
    }
   ],
   "source": [
    "def gen_train_test(frac_train = 0.3, num = 113, seed=0):\n",
    "    # Generate train and test split\n",
    "    pairs = [(i, j, num) for i in range(num) for j in range(num)]\n",
    "    random.seed(seed)\n",
    "    random.shuffle(pairs)\n",
    "    div = int(frac_train*len(pairs))\n",
    "    return pairs[:div], pairs[div:]\n",
    "\n",
    "train_x, test_x = gen_train_test(frac_train, 113, seed=0)\n",
    "print(len(train_x), len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 67, 113)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[ 18,  34, 113],\n",
      "        [ 10,  83, 113],\n",
      "        [ 55,  28, 113],\n",
      "        [ 25,  51, 113],\n",
      "        [ 63,  65, 113],\n",
      "        [ 14, 104, 113],\n",
      "        [ 27,  56, 113],\n",
      "        [ 35,  77, 113],\n",
      "        [ 41,  72, 113],\n",
      "        [ 17, 111, 113]], device='cuda:0')\n",
      "tensor([ 52,  93,  83,  76,  15,   5,  83, 112,   0,  15], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def collate_batch(batch):\n",
    "    label_list = [(i+j) % num for (i,j,num) in batch]\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    input_list = [torch.tensor(x) for x in batch]\n",
    "    input_list = torch.cat(input_list).view(len(label_list), -1)\n",
    "    return input_list.to(DEVICE), label_list.to(DEVICE)\n",
    "\n",
    "# one batch per loader\n",
    "train_loader = DataLoader(train_x, batch_size=len(train_x), collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_x, batch_size=len(test_x), collate_fn=collate_batch)\n",
    "\n",
    "print(len(train_loader))\n",
    "for x, y in train_loader:\n",
    "    print(x[:10])\n",
    "    print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch_x = batch[0]\n",
    "batch_y = batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3830, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 18,  34, 113], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk = load_model(\"grokker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embed): Embed()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (blocks): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): Attention()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226816"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(gk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 114])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gk(batch_x[:1]).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test transformer init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dots.models as dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3 #@param\n",
    "weight_decay = 0.1 #@param \n",
    "p=113 #@param\n",
    "d_model = 128 #@param\n",
    "fn_name = 'add' #@param ['add', 'subtract', 'x2xyy2','rand']\n",
    "frac_train = 0.3 #@param\n",
    "NUM_EPOCHS = 100000 #@param\n",
    "save_models = False #@param\n",
    "save_every = 1000 #@param\n",
    "# Stop training when test loss is <stopping_thresh\n",
    "stopping_thresh = -1 #@param\n",
    "seed = 0 #@param\n",
    "num_layers = 1\n",
    "batch_style = 'full'\n",
    "d_vocab = p+1 \n",
    "n_ctx = 3\n",
    "d_mlp = 4*d_model\n",
    "num_heads = 4\n",
    "assert d_model % num_heads == 0\n",
    "d_head = d_model//num_heads\n",
    "act_type = 'ReLU' #@param ['ReLU', 'GeLU']\n",
    "#batch_size = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = dm.Transformer(\n",
    "    num_layers, d_vocab, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, use_ln=True\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226816"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([226816])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.get_param_tensor().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lrl34/.conda/envs/dots/lib/python3.10/site-packages/torch/nn/utils/stateless.py:216: UserWarning: This API is deprecated as of PyTorch 2.0 and will be removed in a future version of PyTorch. Please use torch.func.functional_call instead which is a drop-in replacement for this API.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mjac = matrix_jacobian(transformer, batch_x[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11400, 226816])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mjac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10342809600"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mjac.element_size() * mjac.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu117'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = transformer.jacobian_matrix_rank(batch_x[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1275, device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
