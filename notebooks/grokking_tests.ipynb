{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import math\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.utils.data as tdata\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "from dots.training import *\n",
    "from dots.trainhooks import *\n",
    "from dots.models import MLP\n",
    "from dots.dots import *\n",
    "from dots.utils import get_device\n",
    "from dots.plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import functools\n",
    "import itertools\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import einops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.W_E = nn.Parameter(torch.randn(d_model, d_vocab)/np.sqrt(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.einsum('dbp -> bpd', self.W_E[:, x])\n",
    "\n",
    "class Unembed(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.W_U = nn.Parameter(torch.randn(d_model, d_vocab)/np.sqrt(d_vocab))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (x @ self.W_U)\n",
    "\n",
    "\n",
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, max_ctx, d_model):\n",
    "        super().__init__()\n",
    "        self.W_pos = nn.Parameter(torch.randn(max_ctx, d_model)/np.sqrt(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x+self.W_pos[:x.shape[-2]]\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, epsilon = 1e-4, model=[None]):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.w_ln = nn.Parameter(torch.ones(d_model))\n",
    "        self.b_ln = nn.Parameter(torch.zeros(d_model))\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.model[0].use_ln:\n",
    "            x = x - x.mean(axis=-1)[..., None]\n",
    "            x = x / (x.std(axis=-1)[..., None] + self.epsilon)\n",
    "            x = x * self.w_ln\n",
    "            x = x + self.b_ln\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_head, n_ctx, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.W_K = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_Q = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_V = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_O = nn.Parameter(torch.randn(d_model, d_head * num_heads)/np.sqrt(d_model))\n",
    "        self.register_buffer('mask', torch.tril(torch.ones((n_ctx, n_ctx))))\n",
    "        self.d_head = d_head\n",
    "        \n",
    "    def forward(self, x):\n",
    "        k = torch.einsum('ihd,bpd->biph', self.W_K, x)\n",
    "        q = torch.einsum('ihd,bpd->biph', self.W_Q, x)\n",
    "        v = torch.einsum('ihd,bpd->biph', self.W_V, x)\n",
    "        attn_scores_pre = torch.einsum('biph,biqh->biqp', k, q)\n",
    "        attn_scores_masked = torch.tril(attn_scores_pre) - 1e10 * (1 - self.mask[:x.shape[-2], :x.shape[-2]])\n",
    "        attn_matrix = F.softmax(attn_scores_masked/np.sqrt(self.d_head), dim=-1)\n",
    "        z = torch.einsum('biph,biqp->biqh', v, attn_matrix)\n",
    "        z_flat = einops.rearrange(z, 'b i q h -> b q (i h)')\n",
    "        out = torch.einsum('df,bqf->bqd', self.W_O, z_flat)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, d_mlp, act_type, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.W_in = nn.Parameter(torch.randn(d_mlp, d_model)/np.sqrt(d_model))\n",
    "        self.b_in = nn.Parameter(torch.zeros(d_mlp))\n",
    "        self.W_out = nn.Parameter(torch.randn(d_model, d_mlp)/np.sqrt(d_model))\n",
    "        self.b_out = nn.Parameter(torch.zeros(d_model))\n",
    "        self.act_type = act_type\n",
    "        # self.ln = LayerNorm(d_mlp, model=self.model)\n",
    "        assert act_type in ['ReLU', 'GeLU']\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.einsum('md,bpd->bpm', self.W_in, x) + self.b_in\n",
    "        \n",
    "        if self.act_type=='ReLU':\n",
    "            x = F.relu(x)\n",
    "        elif self.act_type=='GeLU':\n",
    "            x = F.gelu(x)\n",
    "        \n",
    "        x = torch.einsum('dm,bpm->bpd', self.W_out, x) + self.b_out\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        # self.ln1 = LayerNorm(d_model, model=self.model)\n",
    "        self.attn = Attention(d_model, num_heads, d_head, n_ctx, model=self.model)\n",
    "        # self.ln2 = LayerNorm(d_model, model=self.model)\n",
    "        self.mlp = MLP(d_model, d_mlp, act_type, model=self.model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(x)\n",
    "        x = x + self.mlp((x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_vocab, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, use_ln=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = Embed(d_vocab, d_model)\n",
    "        self.pos_embed = PosEmbed(n_ctx, d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model=[self]) for i in range(num_layers)])\n",
    "        # self.ln = LayerNorm(d_model, model=[self])\n",
    "        self.unembed = Unembed(d_vocab, d_model)\n",
    "        self.use_ln = use_ln\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.pos_embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        # x = self.ln(x)\n",
    "        x = self.unembed(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr=1e-3 #@param\n",
    "weight_decay = 0.1 #@param \n",
    "p=113 #@param\n",
    "d_model = 128 #@param\n",
    "fn_name = 'add' #@param ['add', 'subtract', 'x2xyy2','rand']\n",
    "frac_train = 0.3 #@param\n",
    "NUM_EPOCHS = 100000 #@param\n",
    "save_models = False #@param\n",
    "save_every = 1000 #@param\n",
    "# Stop training when test loss is <stopping_thresh\n",
    "stopping_thresh = -1 #@param\n",
    "seed = 0 #@param\n",
    "num_layers = 1\n",
    "batch_style = 'full'\n",
    "d_vocab = p+1 \n",
    "n_ctx = 3\n",
    "d_mlp = 4*d_model\n",
    "num_heads = 4\n",
    "assert d_model % num_heads == 0\n",
    "d_head = d_model//num_heads\n",
    "act_type = 'ReLU' #@param ['ReLU', 'GeLU']\n",
    "#batch_size = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3830 8939\n"
     ]
    }
   ],
   "source": [
    "def gen_train_test(frac_train = 0.3, num = 113, seed=0):\n",
    "    # Generate train and test split\n",
    "    pairs = [(i, j, num) for i in range(num) for j in range(num)]\n",
    "    random.seed(seed)\n",
    "    random.shuffle(pairs)\n",
    "    div = int(frac_train*len(pairs))\n",
    "    return pairs[:div], pairs[div:]\n",
    "\n",
    "train_x, test_x = gen_train_test(frac_train, 113, seed=0)\n",
    "print(len(train_x), len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 67, 113)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[ 18,  34, 113],\n",
      "        [ 10,  83, 113],\n",
      "        [ 55,  28, 113],\n",
      "        [ 25,  51, 113],\n",
      "        [ 63,  65, 113],\n",
      "        [ 14, 104, 113],\n",
      "        [ 27,  56, 113],\n",
      "        [ 35,  77, 113],\n",
      "        [ 41,  72, 113],\n",
      "        [ 17, 111, 113]], device='cuda:0')\n",
      "tensor([ 52,  93,  83,  76,  15,   5,  83, 112,   0,  15], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def collate_batch(batch):\n",
    "    label_list = [(i+j) % num for (i,j,num) in batch]\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    input_list = [torch.tensor(x) for x in batch]\n",
    "    input_list = torch.cat(input_list).view(len(label_list), -1)\n",
    "    return input_list.to(DEVICE), label_list.to(DEVICE)\n",
    "\n",
    "# one batch per loader\n",
    "train_loader = DataLoader(train_x, batch_size=len(train_x), collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_x, batch_size=len(test_x), collate_fn=collate_batch)\n",
    "\n",
    "print(len(train_loader))\n",
    "for x, y in train_loader:\n",
    "    print(x[:10])\n",
    "    print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch_x = batch[0]\n",
    "batch_y = batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3830, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 18,  34, 113], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk = load_model(\"grokker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embed): Embed()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (blocks): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): Attention()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226816"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(gk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 114])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gk(batch_x[:1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Transformer' object has no attribute 'get_param_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m jacobian_matrix_rank(gk, batch_x[:\u001b[39m10\u001b[39;49m])\n",
      "File \u001b[0;32m~/dots/dots/notebooks/../dots/dots.py:28\u001b[0m, in \u001b[0;36mjacobian_matrix_rank\u001b[0;34m(model, inputs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjacobian_matrix_rank\u001b[39m(model, inputs):\n\u001b[0;32m---> 28\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mmatrix_rank(matrix_jacobian(model, inputs))\n",
      "File \u001b[0;32m~/dots/dots/notebooks/../dots/dots.py:22\u001b[0m, in \u001b[0;36mmatrix_jacobian\u001b[0;34m(model, inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmatrix_jacobian\u001b[39m(model, inputs):\n\u001b[0;32m---> 22\u001b[0m     J \u001b[39m=\u001b[39m jacobian(model, inputs)\n\u001b[1;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(J\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m     24\u001b[0m         J \u001b[39m=\u001b[39m rearrange(J, \u001b[39m\"\u001b[39m\u001b[39mb o p -> (b o) p\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dots/dots/notebooks/../dots/dots.py:19\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(model, inputs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     param_state_dict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mparam_tensor_to_state_dict(params_tensor)\n\u001b[1;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m stateless\u001b[39m.\u001b[39mfunctional_call(model, param_state_dict, inputs)\n\u001b[1;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mjacobian(\n\u001b[0;32m---> 19\u001b[0m     input_as_fn_of_params, model\u001b[39m.\u001b[39;49mget_param_tensor())\n",
      "File \u001b[0;32m~/.conda/envs/dots/lib/python3.10/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Transformer' object has no attribute 'get_param_tensor'"
     ]
    }
   ],
   "source": [
    "jacobian_matrix_rank(gk, batch_x[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test transformer init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dots.models as dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3 #@param\n",
    "weight_decay = 0.1 #@param \n",
    "p=113 #@param\n",
    "d_model = 128 #@param\n",
    "fn_name = 'add' #@param ['add', 'subtract', 'x2xyy2','rand']\n",
    "frac_train = 0.3 #@param\n",
    "NUM_EPOCHS = 100000 #@param\n",
    "save_models = False #@param\n",
    "save_every = 1000 #@param\n",
    "# Stop training when test loss is <stopping_thresh\n",
    "stopping_thresh = -1 #@param\n",
    "seed = 0 #@param\n",
    "num_layers = 1\n",
    "batch_style = 'full'\n",
    "d_vocab = p+1 \n",
    "n_ctx = 3\n",
    "d_mlp = 4*d_model\n",
    "num_heads = 4\n",
    "assert d_model % num_heads == 0\n",
    "d_head = d_model//num_heads\n",
    "act_type = 'ReLU' #@param ['ReLU', 'GeLU']\n",
    "#batch_size = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = dm.Transformer(\n",
    "    num_layers, d_vocab, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, use_ln=True\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226816"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([226816])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.get_param_tensor().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "mjac = matrix_jacobian(transformer, batch_x[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11400, 226816])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mjac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10342809600"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mjac.element_size() * mjac.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cusolver error: CUSOLVER_STATUS_INVALID_VALUE, when calling `cusolverDnSgesvdj_bufferSize(handle, jobz, econ, m, n, A, lda, S, U, ldu, V, ldv, lwork, params)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m r \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mjacobian_matrix_rank(batch_x[:\u001b[39m100\u001b[39;49m])\n",
      "File \u001b[0;32m~/dots/dots/notebooks/../dots/dots.py:138\u001b[0m, in \u001b[0;36mJModule.jacobian_matrix_rank\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjacobian_matrix_rank\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mreturn\u001b[39;00m jacobian_matrix_rank(\u001b[39mself\u001b[39;49m, inputs)\n",
      "File \u001b[0;32m~/dots/dots/notebooks/../dots/dots.py:30\u001b[0m, in \u001b[0;36mjacobian_matrix_rank\u001b[0;34m(model, inputs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjacobian_matrix_rank\u001b[39m(model, inputs):\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mmatrix_rank(matrix_jacobian(model, inputs))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cusolver error: CUSOLVER_STATUS_INVALID_VALUE, when calling `cusolverDnSgesvdj_bufferSize(handle, jobz, econ, m, n, A, lda, S, U, ldu, V, ldv, lwork, params)`"
     ]
    }
   ],
   "source": [
    "r = transformer.jacobian_matrix_rank(batch_x[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(114, device='cuda:0')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cusolver error: CUSOLVER_STATUS_INVALID_VALUE, when calling `cusolverDnSgesvdj_bufferSize(handle, jobz, econ, m, n, A, lda, S, U, ldu, V, ldv, lwork, params)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m U, S, V \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mjacobian_svd(batch_x[:\u001b[39m100\u001b[39;49m])\n",
      "File \u001b[0;32m~/dots/dots/notebooks/../dots/dots.py:154\u001b[0m, in \u001b[0;36mJModule.jacobian_svd\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjacobian_svd\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m--> 154\u001b[0m     \u001b[39mreturn\u001b[39;00m jacobian_svd(\u001b[39mself\u001b[39;49m, inputs)\n",
      "File \u001b[0;32m~/dots/dots/notebooks/../dots/dots.py:73\u001b[0m, in \u001b[0;36mjacobian_svd\u001b[0;34m(model, inputs, heuristic_trim)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjacobian_svd\u001b[39m(model, inputs, heuristic_trim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 73\u001b[0m     U, S, Vh \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49msvd(\n\u001b[1;32m     74\u001b[0m         matrix_jacobian(model, inputs),\n\u001b[1;32m     75\u001b[0m         full_matrices\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m     \u001b[39mif\u001b[39;00m heuristic_trim:\n\u001b[1;32m     78\u001b[0m         i_cut \u001b[39m=\u001b[39m first_occurrence_of_equal_adjacents(S, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cusolver error: CUSOLVER_STATUS_INVALID_VALUE, when calling `cusolverDnSgesvdj_bufferSize(handle, jobz, econ, m, n, A, lda, S, U, ldu, V, ldv, lwork, params)`"
     ]
    }
   ],
   "source": [
    "U, S, V = transformer.jacobian_svd(batch_x[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
